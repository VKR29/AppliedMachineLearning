{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9i8Xrn-826C"
      },
      "source": [
        "# SMS Spam Classification - Model Training\n",
        "This notebook contains functions to train, evaluate, and tune models for SMS spam classification."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score,\n",
        "    f1_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "_UZYxWBA-cEU"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Loading, Preprocessing, Spliting and Storing Data"
      ],
      "metadata": {
        "id": "2liV9KB2-d4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load SMS spam collection data from file.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Path to the SMS spam collection file\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with columns 'label' and 'message'\n",
        "    \"\"\"\n",
        "    # Read tab-separated file\n",
        "    df = pd.read_csv(file_path, sep='\\t', names=['label', 'message'], encoding='utf-8')\n",
        "\n",
        "    print(f\"Data loaded successfully!\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "WibGQoJ6-fwL"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Preprocess the SMS data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        Raw dataframe with 'label' and 'message' columns\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Preprocessed dataframe\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert labels to binary (0 = ham, 1 = spam)\n",
        "    df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates(subset='message', keep='first')\n",
        "\n",
        "    # Remove null values if any\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Reset index\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    print(f\"Preprocessing complete!\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "fa_rT6cO-j0V"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(df, train_size=0.7, val_size=0.15, test_size=0.15, random_state=42):\n",
        "    \"\"\"\n",
        "    Split data into train, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        Preprocessed dataframe\n",
        "    train_size : float\n",
        "        Proportion of data for training (default: 0.7)\n",
        "    val_size : float\n",
        "        Proportion of data for validation (default: 0.15)\n",
        "    test_size : float\n",
        "        Proportion of data for testing (default: 0.15)\n",
        "    random_state : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (train_df, val_df, test_df)\n",
        "    \"\"\"\n",
        "    assert abs(train_size + val_size + test_size - 1.0) < 1e-6, \"Sizes must sum to 1.0\"\n",
        "\n",
        "    # First split: separate test set\n",
        "    train_val_df, test_df = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Second split: separate train and validation\n",
        "    val_ratio = val_size / (train_size + val_size)\n",
        "    train_df, val_df = train_test_split(\n",
        "        train_val_df,\n",
        "        test_size=val_ratio,\n",
        "        random_state=random_state,\n",
        "        stratify=train_val_df['label']\n",
        "    )\n",
        "\n",
        "    print(f\"Data split complete!\")\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "9vwmUrzT-m3r"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_splits(train_df, val_df, test_df,\n",
        "                 train_path='train.csv',\n",
        "                 val_path='validation.csv',\n",
        "                 test_path='test.csv'):\n",
        "    \"\"\"\n",
        "    Store train, validation, and test splits to CSV files.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_df, val_df, test_df : pd.DataFrame\n",
        "        DataFrames to store\n",
        "    train_path, val_path, test_path : str\n",
        "        File paths for storing the splits\n",
        "    \"\"\"\n",
        "    train_df.to_csv(train_path, index=False)\n",
        "    val_df.to_csv(val_path, index=False)\n",
        "    test_df.to_csv(test_path, index=False)\n",
        "\n",
        "    print(f\"Splits saved successfully!\")"
      ],
      "metadata": {
        "id": "fzZ7wfOI-7_M"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHhWnkyS826E"
      },
      "source": [
        "## 2. Load Data Splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "GolUCDho826E"
      },
      "outputs": [],
      "source": [
        "def load_splits(train_path='train.csv', val_path='validation.csv', test_path='test.csv'):\n",
        "    \"\"\"\n",
        "    Load train, validation, and test splits.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (train_df, val_df, test_df)\n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    print(f\"Data loaded successfully!\")\n",
        "    print(f\"Train: {len(train_df)}, Validation: {len(val_df)}, Test: {len(test_df)}\")\n",
        "\n",
        "    return train_df, val_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6mH30yr826E"
      },
      "source": [
        "## 3. Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "bJaDEDLW826F"
      },
      "outputs": [],
      "source": [
        "def create_features(train_df, val_df, test_df, max_features=3000):\n",
        "    \"\"\"\n",
        "    Create TF-IDF features from text data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_df, val_df, test_df : pd.DataFrame\n",
        "        DataFrames with 'message' column\n",
        "    max_features : int\n",
        "        Maximum number of features for TF-IDF\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (X_train, X_val, X_test, y_train, y_val, y_test, vectorizer)\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(max_features=max_features, lowercase=True, stop_words='english')\n",
        "\n",
        "    # Fit on train and transform all sets\n",
        "    X_train = vectorizer.fit_transform(train_df['message'])\n",
        "    X_val = vectorizer.transform(val_df['message'])\n",
        "    X_test = vectorizer.transform(test_df['message'])\n",
        "\n",
        "    y_train = train_df['label'].values\n",
        "    y_val = val_df['label'].values\n",
        "    y_test = test_df['label'].values\n",
        "\n",
        "    print(f\"Features created!\")\n",
        "    print(f\"Feature matrix shape: {X_train.shape}\")\n",
        "    print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAUGl_QB826F"
      },
      "source": [
        "## 4. Fit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "K0uNxytq826F"
      },
      "outputs": [],
      "source": [
        "def fit_model(model, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Fit a model on training data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : sklearn model\n",
        "        Model to train\n",
        "    X_train : array-like\n",
        "        Training features\n",
        "    y_train : array-like\n",
        "        Training labels\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model\n",
        "        Fitted model\n",
        "    \"\"\"\n",
        "    print(f\"Training {model.__class__.__name__}...\")\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"Training complete!\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL7vgtCq826F"
      },
      "source": [
        "## 5. Score Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "m5mELWrM826F"
      },
      "outputs": [],
      "source": [
        "def score_model(model, X, y):\n",
        "    \"\"\"\n",
        "    Get predictions and basic accuracy score.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : sklearn model\n",
        "        Trained model\n",
        "    X : array-like\n",
        "        Features\n",
        "    y : array-like\n",
        "        True labels\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (predictions, accuracy)\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(X)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    return y_pred, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqq6xvNC826F"
      },
      "source": [
        "## 6. Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "jGZHeksf826F"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, set_name=''):\n",
        "    \"\"\"\n",
        "    Evaluate model predictions with detailed metrics.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        True labels\n",
        "    y_pred : array-like\n",
        "        Predicted labels\n",
        "    set_name : str\n",
        "        Name of dataset (e.g., 'Train', 'Validation', 'Test')\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing all metrics\n",
        "    \"\"\"\n",
        "    metrics = {\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred),\n",
        "        'recall': recall_score(y_true, y_pred),\n",
        "        'f1': f1_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"{set_name} Set Evaluation\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
        "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
        "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
        "    print(f\"F1-Score:  {metrics['f1']:.4f}\")\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=['ham', 'spam']))\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrSTOfFo826G"
      },
      "source": [
        "## 7. Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "8iuuIOxy826G"
      },
      "outputs": [],
      "source": [
        "def validate_model(model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Fit model on train and evaluate on both train and validation.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : sklearn model\n",
        "        Model to validate\n",
        "    X_train, y_train : array-like\n",
        "        Training data\n",
        "    X_val, y_val : array-like\n",
        "        Validation data\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (train_metrics, val_metrics)\n",
        "    \"\"\"\n",
        "    # Fit model\n",
        "    model = fit_model(model, X_train, y_train)\n",
        "\n",
        "    # Score on train\n",
        "    y_train_pred, train_acc = score_model(model, X_train, y_train)\n",
        "    print(f\"\\nTrain Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "    # Score on validation\n",
        "    y_val_pred, val_acc = score_model(model, X_val, y_val)\n",
        "    print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    # Evaluate\n",
        "    train_metrics = evaluate_model(y_train, y_train_pred, 'Train')\n",
        "    val_metrics = evaluate_model(y_val, y_val_pred, 'Validation')\n",
        "\n",
        "    return train_metrics, val_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbykZqEd826G"
      },
      "source": [
        "## 8. Tune Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "hYVY_B74826G"
      },
      "outputs": [],
      "source": [
        "def tune_hyperparameters(model, param_grid, X_train, y_train, cv=3):\n",
        "    \"\"\"\n",
        "    Perform grid search for hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : sklearn model\n",
        "        Base model to tune\n",
        "    param_grid : dict\n",
        "        Parameter grid for grid search\n",
        "    X_train, y_train : array-like\n",
        "        Training data\n",
        "    cv : int\n",
        "        Number of cross-validation folds\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model\n",
        "        Best model from grid search\n",
        "    \"\"\"\n",
        "    print(f\"\\nTuning hyperparameters for {model.__class__.__name__}...\")\n",
        "    print(f\"Parameter grid: {param_grid}\")\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        model, param_grid, cv=cv, scoring='f1',\n",
        "        n_jobs=-1, verbose=1\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation F1 score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    return grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJRg9kBZ826G"
      },
      "source": [
        "## 9. Train and Evaluate Benchmark Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "cPQT5mPB826G"
      },
      "outputs": [],
      "source": [
        "def train_benchmark_models(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Train three benchmark models and evaluate on test set.\n",
        "\n",
        "    Models:\n",
        "    1. Multinomial Naive Bayes\n",
        "    2. Logistic Regression\n",
        "    3. Random Forest\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing models and their test metrics\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Model 1: Multinomial Naive Bayes\n",
        "    print(\"\\n\" + \"#\"*60)\n",
        "    print(\"MODEL 1: MULTINOMIAL NAIVE BAYES\")\n",
        "    print(\"#\"*60)\n",
        "\n",
        "    nb_model = MultinomialNB()\n",
        "    nb_train_metrics, nb_val_metrics = validate_model(nb_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Test evaluation\n",
        "    y_test_pred, test_acc = score_model(nb_model, X_test, y_test)\n",
        "    nb_test_metrics = evaluate_model(y_test, y_test_pred, 'Test')\n",
        "\n",
        "    results['Naive Bayes'] = {\n",
        "        'model': nb_model,\n",
        "        'train_metrics': nb_train_metrics,\n",
        "        'val_metrics': nb_val_metrics,\n",
        "        'test_metrics': nb_test_metrics\n",
        "    }\n",
        "\n",
        "    # Model 2: Logistic Regression\n",
        "    print(\"\\n\" + \"#\"*60)\n",
        "    print(\"MODEL 2: LOGISTIC REGRESSION\")\n",
        "    print(\"#\"*60)\n",
        "\n",
        "    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    lr_train_metrics, lr_val_metrics = validate_model(lr_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Test evaluation\n",
        "    y_test_pred, test_acc = score_model(lr_model, X_test, y_test)\n",
        "    lr_test_metrics = evaluate_model(y_test, y_test_pred, 'Test')\n",
        "\n",
        "    results['Logistic Regression'] = {\n",
        "        'model': lr_model,\n",
        "        'train_metrics': lr_train_metrics,\n",
        "        'val_metrics': lr_val_metrics,\n",
        "        'test_metrics': lr_test_metrics\n",
        "    }\n",
        "\n",
        "    # Model 3: Random Forest\n",
        "    print(\"\\n\" + \"#\"*60)\n",
        "    print(\"MODEL 3: RANDOM FOREST\")\n",
        "    print(\"#\"*60)\n",
        "\n",
        "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_train_metrics, rf_val_metrics = validate_model(rf_model, X_train, y_train, X_val, y_val)\n",
        "\n",
        "    # Test evaluation\n",
        "    y_test_pred, test_acc = score_model(rf_model, X_test, y_test)\n",
        "    rf_test_metrics = evaluate_model(y_test, y_test_pred, 'Test')\n",
        "\n",
        "    results['Random Forest'] = {\n",
        "        'model': rf_model,\n",
        "        'train_metrics': rf_train_metrics,\n",
        "        'val_metrics': rf_val_metrics,\n",
        "        'test_metrics': rf_test_metrics\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEN9Eeqt826H"
      },
      "source": [
        "## 10. Select Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "yo_9vRXP826H"
      },
      "outputs": [],
      "source": [
        "def select_best_model(results):\n",
        "    \"\"\"\n",
        "    Select the best model based on test F1 score.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : dict\n",
        "        Dictionary containing model results\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (best_model_name, best_model)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MODEL COMPARISON (TEST SET)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    comparison = []\n",
        "    for name, result in results.items():\n",
        "        metrics = result['test_metrics']\n",
        "        comparison.append({\n",
        "            'Model': name,\n",
        "            'Accuracy': metrics['accuracy'],\n",
        "            'Precision': metrics['precision'],\n",
        "            'Recall': metrics['recall'],\n",
        "            'F1-Score': metrics['f1']\n",
        "        })\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison)\n",
        "    comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
        "    print(comparison_df.to_string(index=False))\n",
        "\n",
        "    best_model_name = comparison_df.iloc[0]['Model']\n",
        "    best_model = results[best_model_name]['model']\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"BEST MODEL: {best_model_name}\")\n",
        "    print(f\"Test F1-Score: {comparison_df.iloc[0]['F1-Score']:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return best_model_name, best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E5m9ib0826H"
      },
      "source": [
        "## 11. Run Complete Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "oOn2dig9826H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf0d8e6-ef26-489e-ffd7-d4e9b6d38b2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "Preprocessing complete!\n",
            "Data split complete!\n",
            "Splits saved successfully!\n",
            "Data loaded successfully!\n",
            "Train: 3617, Validation: 776, Test: 776\n",
            "Features created!\n",
            "Feature matrix shape: (3617, 3000)\n",
            "Vocabulary size: 3000\n",
            "\n",
            "############################################################\n",
            "MODEL 1: MULTINOMIAL NAIVE BAYES\n",
            "############################################################\n",
            "Training MultinomialNB...\n",
            "Training complete!\n",
            "\n",
            "Train Accuracy: 0.9865\n",
            "Validation Accuracy: 0.9704\n",
            "\n",
            "==================================================\n",
            "Train Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9865\n",
            "Precision: 1.0000\n",
            "Recall:    0.8928\n",
            "F1-Score:  0.9434\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3160    0]\n",
            " [  49  408]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99      3160\n",
            "        spam       1.00      0.89      0.94       457\n",
            "\n",
            "    accuracy                           0.99      3617\n",
            "   macro avg       0.99      0.95      0.97      3617\n",
            "weighted avg       0.99      0.99      0.99      3617\n",
            "\n",
            "\n",
            "==================================================\n",
            "Validation Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9704\n",
            "Precision: 1.0000\n",
            "Recall:    0.7653\n",
            "F1-Score:  0.8671\n",
            "\n",
            "Confusion Matrix:\n",
            "[[678   0]\n",
            " [ 23  75]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.98       678\n",
            "        spam       1.00      0.77      0.87        98\n",
            "\n",
            "    accuracy                           0.97       776\n",
            "   macro avg       0.98      0.88      0.93       776\n",
            "weighted avg       0.97      0.97      0.97       776\n",
            "\n",
            "\n",
            "==================================================\n",
            "Test Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9794\n",
            "Precision: 1.0000\n",
            "Recall:    0.8367\n",
            "F1-Score:  0.9111\n",
            "\n",
            "Confusion Matrix:\n",
            "[[678   0]\n",
            " [ 16  82]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99       678\n",
            "        spam       1.00      0.84      0.91        98\n",
            "\n",
            "    accuracy                           0.98       776\n",
            "   macro avg       0.99      0.92      0.95       776\n",
            "weighted avg       0.98      0.98      0.98       776\n",
            "\n",
            "\n",
            "############################################################\n",
            "MODEL 2: LOGISTIC REGRESSION\n",
            "############################################################\n",
            "Training LogisticRegression...\n",
            "Training complete!\n",
            "\n",
            "Train Accuracy: 0.9693\n",
            "Validation Accuracy: 0.9523\n",
            "\n",
            "==================================================\n",
            "Train Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9693\n",
            "Precision: 0.9915\n",
            "Recall:    0.7637\n",
            "F1-Score:  0.8628\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3157    3]\n",
            " [ 108  349]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.98      3160\n",
            "        spam       0.99      0.76      0.86       457\n",
            "\n",
            "    accuracy                           0.97      3617\n",
            "   macro avg       0.98      0.88      0.92      3617\n",
            "weighted avg       0.97      0.97      0.97      3617\n",
            "\n",
            "\n",
            "==================================================\n",
            "Validation Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9523\n",
            "Precision: 1.0000\n",
            "Recall:    0.6224\n",
            "F1-Score:  0.7673\n",
            "\n",
            "Confusion Matrix:\n",
            "[[678   0]\n",
            " [ 37  61]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      1.00      0.97       678\n",
            "        spam       1.00      0.62      0.77        98\n",
            "\n",
            "    accuracy                           0.95       776\n",
            "   macro avg       0.97      0.81      0.87       776\n",
            "weighted avg       0.95      0.95      0.95       776\n",
            "\n",
            "\n",
            "==================================================\n",
            "Test Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9601\n",
            "Precision: 0.9718\n",
            "Recall:    0.7041\n",
            "F1-Score:  0.8166\n",
            "\n",
            "Confusion Matrix:\n",
            "[[676   2]\n",
            " [ 29  69]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      1.00      0.98       678\n",
            "        spam       0.97      0.70      0.82        98\n",
            "\n",
            "    accuracy                           0.96       776\n",
            "   macro avg       0.97      0.85      0.90       776\n",
            "weighted avg       0.96      0.96      0.96       776\n",
            "\n",
            "\n",
            "############################################################\n",
            "MODEL 3: RANDOM FOREST\n",
            "############################################################\n",
            "Training RandomForestClassifier...\n",
            "Training complete!\n",
            "\n",
            "Train Accuracy: 0.9997\n",
            "Validation Accuracy: 0.9716\n",
            "\n",
            "==================================================\n",
            "Train Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9997\n",
            "Precision: 1.0000\n",
            "Recall:    0.9978\n",
            "F1-Score:  0.9989\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3160    0]\n",
            " [   1  456]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       1.00      1.00      1.00      3160\n",
            "        spam       1.00      1.00      1.00       457\n",
            "\n",
            "    accuracy                           1.00      3617\n",
            "   macro avg       1.00      1.00      1.00      3617\n",
            "weighted avg       1.00      1.00      1.00      3617\n",
            "\n",
            "\n",
            "==================================================\n",
            "Validation Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9716\n",
            "Precision: 0.9872\n",
            "Recall:    0.7857\n",
            "F1-Score:  0.8750\n",
            "\n",
            "Confusion Matrix:\n",
            "[[677   1]\n",
            " [ 21  77]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.98       678\n",
            "        spam       0.99      0.79      0.88        98\n",
            "\n",
            "    accuracy                           0.97       776\n",
            "   macro avg       0.98      0.89      0.93       776\n",
            "weighted avg       0.97      0.97      0.97       776\n",
            "\n",
            "\n",
            "==================================================\n",
            "Test Set Evaluation\n",
            "==================================================\n",
            "Accuracy:  0.9768\n",
            "Precision: 0.9878\n",
            "Recall:    0.8265\n",
            "F1-Score:  0.9000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[677   1]\n",
            " [ 17  81]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.98      1.00      0.99       678\n",
            "        spam       0.99      0.83      0.90        98\n",
            "\n",
            "    accuracy                           0.98       776\n",
            "   macro avg       0.98      0.91      0.94       776\n",
            "weighted avg       0.98      0.98      0.98       776\n",
            "\n",
            "\n",
            "============================================================\n",
            "MODEL COMPARISON (TEST SET)\n",
            "============================================================\n",
            "              Model  Accuracy  Precision   Recall  F1-Score\n",
            "        Naive Bayes  0.979381   1.000000 0.836735  0.911111\n",
            "      Random Forest  0.976804   0.987805 0.826531  0.900000\n",
            "Logistic Regression  0.960052   0.971831 0.704082  0.816568\n",
            "\n",
            "============================================================\n",
            "BEST MODEL: Naive Bayes\n",
            "Test F1-Score: 0.9111\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "OPTIONAL: HYPERPARAMETER TUNING\n",
            "============================================================\n",
            "\n",
            "Example for Logistic Regression:\n",
            "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
            "tuned_model = tune_hyperparameters(LogisticRegression(max_iter=1000), param_grid, X_train, y_train)\n",
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    # Load data\n",
        "    df = load_data('SMSSpamCollection')\n",
        "\n",
        "    # Preprocess data\n",
        "    df_processed = preprocess_data(df)\n",
        "\n",
        "    # Split data\n",
        "    train_df, val_df, test_df = split_data(df_processed)\n",
        "\n",
        "    # Store splits\n",
        "    store_splits(train_df, val_df, test_df)\n",
        "\n",
        "    # Load data splits\n",
        "    train_df, val_df, test_df = load_splits()\n",
        "\n",
        "    # Create features\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, vectorizer = create_features(\n",
        "        train_df, val_df, test_df\n",
        "    )\n",
        "\n",
        "    # Train benchmark models\n",
        "    results = train_benchmark_models(X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "    # Select best model\n",
        "    best_model_name, best_model = select_best_model(results)\n",
        "\n",
        "    # Optional: Fine-tune best model\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"OPTIONAL: HYPERPARAMETER TUNING\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(\"\\nExample for Logistic Regression:\")\n",
        "    print(\"param_grid = {'C': [0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\")\n",
        "    print(\"tuned_model = tune_hyperparameters(LogisticRegression(max_iter=1000), param_grid, X_train, y_train)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
